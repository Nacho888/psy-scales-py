{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import file_manager\n",
    "import numpy as np\n",
    "\n",
    "y_train = np.load(\"./vectorizers/labels_train.npy\")\n",
    "y_test = np.load(\"./vectorizers/labels_test.npy\")\n",
    "\n",
    "# Dictionary to store all the different file names of the matrices\n",
    "file_names = {\"bow\": {\"train\": [], \"test\": []}, \"tfidf\": {\"train\": [], \"test\": []}}\n",
    "for file in file_manager.files_in_path(\"./vectorizers\"):\n",
    "    if \"labels\" not in file and \"vect\" not in file and \".gitignore\" not in file:\n",
    "        name = file.split(\"_\")\n",
    "        file_names[name[1]][name[0]].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MultinomialNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training ComplementNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training BernoulliNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training SGDClassifier...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Training MultinomialNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training ComplementNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training BernoulliNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training SGDClassifier...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Training MultinomialNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training ComplementNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training BernoulliNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training SGDClassifier...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Training MultinomialNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training ComplementNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training BernoulliNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training SGDClassifier...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Training MultinomialNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training ComplementNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training BernoulliNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training SGDClassifier...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Training MultinomialNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training ComplementNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training BernoulliNB...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Training SGDClassifier...\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   18.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   15.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    9.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   19.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   10.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   17.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   13.9s finished\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "execution_results = []\n",
    "# Define your models here:\n",
    "# [model, *dict* with params to be tested, *list* of ONE item with the scaler to be used]\n",
    "models = [\n",
    "    [MultinomialNB(),\n",
    "        {\"alpha\": [0.1, 0.2, 0.4, 0.6, 0.8, 1]},\n",
    "        []],\n",
    "    [ComplementNB(),\n",
    "        {\"alpha\": [0.1, 0.2, 0.4, 0.6, 0.8, 1]},\n",
    "        []],\n",
    "    [BernoulliNB(),\n",
    "        {\"alpha\": [0.1, 0.2, 0.4, 0.6, 0.8, 1]},\n",
    "        []],\n",
    "    [SGDClassifier(n_jobs=-1, max_iter=1000),\n",
    "        {\"alpha\":[0.0001, 0.001, 0.01]},\n",
    "        [StandardScaler(with_mean=False)]],\n",
    "]\n",
    "\n",
    "for vectorizer_type in file_names.keys():\n",
    "    for train_file, test_file in zip(file_names[vectorizer_type][\"train\"], file_names[vectorizer_type][\"test\"]):\n",
    "        X_train = sparse.load_npz(\"./vectorizers/{}\".format(train_file))\n",
    "        X_test = sparse.load_npz(\"./vectorizers/{}\".format(test_file))\n",
    "\n",
    "        for classifier in models: \n",
    "            print(\"Training {}...\".format(type(classifier[0]).__name__))\n",
    "            begin_time = datetime.datetime.now()\n",
    "            if classifier[1]:\n",
    "                # Train with grid search with cross-validation if parameters are provided\n",
    "                grid = GridSearchCV(estimator=classifier[0], param_grid=classifier[1], scoring=\"accuracy\", verbose=True,\n",
    "                                    cv=3, n_jobs=-1)\n",
    "                # Scale the dataset if needed, else, use the normal dataset\n",
    "                grid.fit(X_train if not classifier[2] else classifier[2][0].fit_transform(X_train), y_train)\n",
    "                # Select the best classifier obtained through grid search\n",
    "                clf = grid.best_estimator_\n",
    "            else:\n",
    "                # Default train if no parameters provided\n",
    "                clf = classifier[0].fit(X_train if not classifier[2] else classifier[2][0].fit_transform(X_train), y_train)\n",
    "            measured_time_train = datetime.datetime.now() - begin_time\n",
    "\n",
    "            # Predict with the test set and labels\n",
    "            begin_time = datetime.datetime.now()\n",
    "            y_pred = clf.predict(X_test if not classifier[2] else classifier[2][0].fit_transform(X_test))\n",
    "            measured_time_test = datetime.datetime.now() - begin_time\n",
    "\n",
    "            # Save all the results in a dictionary\n",
    "            train_results = classification_report(y_train, clf.predict(X_train if not classifier[2] else classifier[2][0].fit_transform(X_train)), target_names=[\"non-depression\", \"depression\"], output_dict=True)\n",
    "            test_results = classification_report(y_test, y_pred, target_names=[\"non-depression\", \"depression\"], output_dict=True)\n",
    "            execution_results.append({\"model\": str(clf),\n",
    "            \"train_accuracy\": train_results[\"accuracy\"],\n",
    "            \"test_accuracy\": test_results[\"accuracy\"],\n",
    "            \"train_results\": train_results,\n",
    "            \"test_results\": test_results,\n",
    "            \"measured_time_train\": str(measured_time_train),\n",
    "            \"measured_time_test\": str(measured_time_test),\n",
    "            \"type\": \"{} with ({}) n-grams\".format(vectorizer_type.upper(), \"-\".join(re.findall(r'\\d+', train_file)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transform the results' dictionary into a pandas dataframe\n",
    "execution_results_df = pd.DataFrame.from_dict(execution_results, orient=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to excel and json\n",
    "execution_results_df.to_excel(\"execution_results.xlsx\")\n",
    "execution_results_df.to_json(\"execution_results.json\", orient=\"records\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38164bit0e230da09833446789e7a9e442994782",
   "language": "python",
   "display_name": "Python 3.8.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}