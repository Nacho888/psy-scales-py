{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reset_variables = False\n",
    "generate_wordcloud = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training datasets...\n",
      "Loading testing datasets...\n",
      "Datasets succesfully loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Introduce the *paths* of the datasets to be preprocessed\n",
    "\n",
    "print(\"Loading training datasets...\")\n",
    "df_train_depression = pd.DataFrame(pd.read_json(\"../datasets/training/dep_training_cut.jsonl\", lines=True))\n",
    "df_train_control = pd.DataFrame(pd.read_json(\"../datasets/training/ctrl_training_cut.jsonl\", lines=True))\n",
    "\n",
    "print(\"Loading testing datasets...\")\n",
    "df_test_depression = pd.DataFrame(pd.read_json(\"../datasets/testing/dep_testing_cut.jsonl\", lines=True))\n",
    "df_test_control = pd.DataFrame(pd.read_json(\"../datasets/testing/ctrl_testing_cut.jsonl\", lines=True))\n",
    "\n",
    "print(\"Datasets succesfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Join dataframes\n",
    "df_train = df_train_depression.append(df_train_control, ignore_index=True)\n",
    "df_test = df_test_depression.append(df_test_control, ignore_index=True)\n",
    "train_size = len(df_train.index)\n",
    "test_size = len(df_test.index)\n",
    "\n",
    "if not generate_wordcloud:\n",
    "    # Remove old dataframes to preserve memory\n",
    "    del df_train_depression, df_train_control, df_test_depression, df_test_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Substitute reddit's keyword '[removed]'\n",
    "df_train[\"title\"] = np.where((df_train.title == \"[removed]\") | (df_train.title == np.nan),'', df_train.title)\n",
    "df_train[\"selftext\"] = np.where((df_train.selftext == \"[removed]\") | (df_train.selftext == np.nan),'', df_train.selftext)\n",
    "df_test[\"title\"] = np.where((df_test.title == \"[removed]\") | (df_test.title == np.nan),'', df_test.title)\n",
    "df_test[\"selftext\"] = np.where((df_test.selftext == \"[removed]\") | (df_test.selftext == np.nan),'', df_test.selftext)\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression[\"title\"] = np.where((df_train_depression.title == \"[removed]\") | (df_train_depression.title == np.nan),'', df_train_depression.title)\n",
    "    df_train_depression[\"selftext\"] = np.where((df_train_depression.selftext == \"[removed]\") | (df_train_depression.selftext == np.nan),'', df_train_depression.selftext)\n",
    "    df_train_control[\"title\"] = np.where((df_train_control.title == \"[removed]\") | (df_train_control.title == np.nan),'', df_train_control.title)\n",
    "    df_train_control[\"selftext\"] = np.where((df_train_control.selftext == \"[removed]\") | (df_train_control.selftext == np.nan),'', df_train_control.selftext)\n",
    "\n",
    "# Remove rows with no text in title neither selftext\n",
    "df_train = df_train[df_train[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "df_test = df_test[df_test[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression = df_train_depression[df_train_depression[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "    df_train_control = df_train_control[df_train_control[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "\n",
    "# Join the rest\n",
    "df_train[\"text\"] = df_train[\"title\"] + \" \"  + df_train[\"selftext\"]\n",
    "df_test[\"text\"] = df_test[\"title\"] + \" \" + df_test[\"selftext\"]\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression[\"text\"] = df_train_depression[\"title\"] + \" \"  + df_train_depression[\"selftext\"]\n",
    "    df_train_control[\"text\"] = df_train_control[\"title\"] + \" \" + df_train_control[\"selftext\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=4940.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b50dac35dfc34281ba2e2da2c67139c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=1000.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24b5c3431d3a4a6998eb0623ff992ef5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=2585.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "844d5a2017c34cd9b7bcd86fe06f8ee0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=2355.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6810298932b1485da6e0fa5b9b29cec9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import cleantext\n",
    "import re\n",
    "import swifter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "\n",
    "try:\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords_set) + r')\\b\\s*')\n",
    "\n",
    "def pre_process(text: str):\n",
    "    \"\"\"\n",
    "    Given a string cleans it up and returns it once processed. Applies:\n",
    "\n",
    "    1) Lowercase\n",
    "    2) Punctuation removal\n",
    "    3) Conversion of unicode symbols\n",
    "    4) Normalize whitespaces\n",
    "    5) Stopwords removal (using nltk's english list)\n",
    "    6) Stem words using a semi-aggressive stemmer (the classical Porter Stemmer)\n",
    "\n",
    "    :param text: str - the string to be processed\n",
    "    :return: str - the processed string\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase, remove punctuation, convert unicode symbols and normalize whitespaces\n",
    "    processed = cleantext.clean(text, lower=True, fix_unicode=True, no_punct=True, no_urls=True)\n",
    "\n",
    "    # Remove stopwords\n",
    "    processed = pattern.sub('', processed)\n",
    "\n",
    "    # Stem words\n",
    "    try:\n",
    "        stemmed_words = [pst.stem(word) for word in word_tokenize(processed)]\n",
    "    except LookupError:\n",
    "        import nltk\n",
    "        nltk.download(\"punkt\")\n",
    "        stemmed_words = [pst.stem(word) for word in word_tokenize(processed)]\n",
    "\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].swifter.apply(lambda x: pre_process(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].swifter.apply(lambda x: pre_process(x))\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression[\"text\"] = df_train_depression[\"text\"].swifter.apply(lambda x: pre_process(x))\n",
    "    df_train_control[\"text\"] = df_train_control[\"text\"].swifter.apply(lambda x: pre_process(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Dataset reduction to only text\n",
    "df_train = df_train[[\"text\", \"depression_related\"]]\n",
    "df_test = df_test[[\"text\", \"depression_related\"]]\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression = df_train_depression[[\"text\", \"depression_related\"]]\n",
    "    df_train_control = df_train_control[[\"text\", \"depression_related\"]]\n",
    "\n",
    "df_train.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                 path_or_buf=\"../datasets/training/proc_training.jsonl\")\n",
    "df_test.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                 path_or_buf=\"../datasets/testing/proc_testing.jsonl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "from scipy import sparse\n",
    "from pandas import DataFrame\n",
    "from typing import Optional\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def vectorize(ngram: tuple, step_name: str, train: DataFrame, test: Optional[DataFrame] = None,\n",
    "              name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Given the n-gram to work with and the train/test datasets, generates a BoW and TFIDF model. In\n",
    "    addition, if told, also generates a wordcloud from the train dataset.\n",
    "\n",
    "    :param ngram: int - the n-gram to work with\n",
    "    :param step_name: str - the n-gram to string\n",
    "    :param train: DataFrame - the train dataset already preprocessed\n",
    "    :param test: DataFrame - the test dataset already preprocessed\n",
    "    :param name: str - the name for the wordcloud image\n",
    "    \"\"\"\n",
    "\n",
    "    is_word_cloud = True if test is None else False\n",
    "    # Bag-of-words\n",
    "    begin_time = datetime.datetime.now()\n",
    "    cv = CountVectorizer(ngram_range=ngram, max_features=10000)\n",
    "    train_bow = cv.fit_transform(train.text.to_list())\n",
    "\n",
    "    if is_word_cloud:\n",
    "        freqs = dict(zip(cv.get_feature_names(), np.asarray(train_bow.sum(axis=0)).ravel()))\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(freqs)\n",
    "        wordcloud.to_file(\"./img/{}_bow_{}.png\".format(name if name is not None else \"\", step_name))\n",
    "        print(\"BOW wordcloud for ngram_range {} generated ({})\".format(step_name, name))\n",
    "    else:\n",
    "        test_bow = cv.transform(test.text.tolist())\n",
    "        # Save both the train and test arrays generated and the vectorizer used\n",
    "        sparse.save_npz(\"./vectorizers/train_bow_{}.npz\".format(step_name), train_bow)\n",
    "        sparse.save_npz(\"./vectorizers/test_bow_{}.npz\".format(step_name), test_bow)\n",
    "        joblib.dump(cv, \"./vectorizers/vect_bow_{}\".format(step_name))\n",
    "\n",
    "        print(\"Total elapsed time (BOW): {} for ngram_range {}\".format(datetime.datetime.now() - begin_time, ngram_range))\n",
    "        print(\"\\tShape of BOW train vector: {}\".format(train_bow.shape))\n",
    "        print(\"\\tShape of BOW test vector: {}\".format(test_bow.shape))\n",
    "\n",
    "    # Term frequency - inverse term frequency\n",
    "    begin_time = datetime.datetime.now()\n",
    "    tf_trans = TfidfTransformer()\n",
    "    train_tfidf = tf_trans.fit_transform(train_bow)\n",
    "\n",
    "    if is_word_cloud:\n",
    "        freqs = dict(zip(cv.get_feature_names(), np.asarray(train_tfidf.sum(axis=0)).ravel()))\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(freqs)\n",
    "        wordcloud.to_file(\"./img/{}_tfidf_{}.png\".format(name if name is not None else \"\", step_name))\n",
    "        print(\"TFIDF wordcloud for ngram_range {} generated ({})\\n\".format(step_name, name))\n",
    "    else:\n",
    "        test_tfidf = tf_trans.transform(test_bow)\n",
    "        # Save both the train and test arrays generated and the vectorizer used\n",
    "        sparse.save_npz(\"./vectorizers/train_tfidf_{}.npz\".format(step_name), train_tfidf)\n",
    "        sparse.save_npz(\"./vectorizers/test_tfidf_{}.npz\".format(step_name), test_tfidf)\n",
    "        joblib.dump(tf_trans, \"./vectorizers/vect_tfidf_{}\".format(step_name))\n",
    "\n",
    "        print(\"Total elapsed time (TFIDF): {} for ngram_range {}\".format(datetime.datetime.now() - begin_time, ngram_range))\n",
    "        print(\"\\tShape of TFIDF train vector: {}\".format(train_tfidf.shape))\n",
    "        print(\"\\tShape of TFIDF test vector: {}\\n\".format(test_tfidf.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time (BOW): 0:00:00.602548 for ngram_range (1, 1)\n",
      "\tShape of BOW train vector: (4940, 10000)\n",
      "\tShape of BOW test vector: (1000, 10000)\n",
      "Total elapsed time (TFIDF): 0:00:00.199181 for ngram_range (1, 1)\n",
      "\tShape of TFIDF train vector: (4940, 10000)\n",
      "\tShape of TFIDF test vector: (1000, 10000)\n",
      "\n",
      "BOW wordcloud for ngram_range 1_1 generated (depression)\n",
      "TFIDF wordcloud for ngram_range 1_1 generated (depression)\n",
      "\n",
      "BOW wordcloud for ngram_range 1_1 generated (reference)\n",
      "TFIDF wordcloud for ngram_range 1_1 generated (reference)\n",
      "\n",
      "Total elapsed time (BOW): 0:00:02.002441 for ngram_range (1, 2)\n",
      "\tShape of BOW train vector: (4940, 10000)\n",
      "\tShape of BOW test vector: (1000, 10000)\n",
      "Total elapsed time (TFIDF): 0:00:00.223203 for ngram_range (1, 2)\n",
      "\tShape of TFIDF train vector: (4940, 10000)\n",
      "\tShape of TFIDF test vector: (1000, 10000)\n",
      "\n",
      "BOW wordcloud for ngram_range 1_2 generated (depression)\n",
      "TFIDF wordcloud for ngram_range 1_2 generated (depression)\n",
      "\n",
      "BOW wordcloud for ngram_range 1_2 generated (reference)\n",
      "TFIDF wordcloud for ngram_range 1_2 generated (reference)\n",
      "\n",
      "Total elapsed time (BOW): 0:00:01.627479 for ngram_range (2, 2)\n",
      "\tShape of BOW train vector: (4940, 10000)\n",
      "\tShape of BOW test vector: (1000, 10000)\n",
      "Total elapsed time (TFIDF): 0:00:00.050046 for ngram_range (2, 2)\n",
      "\tShape of TFIDF train vector: (4940, 10000)\n",
      "\tShape of TFIDF test vector: (1000, 10000)\n",
      "\n",
      "BOW wordcloud for ngram_range 2_2 generated (depression)\n",
      "TFIDF wordcloud for ngram_range 2_2 generated (depression)\n",
      "\n",
      "BOW wordcloud for ngram_range 2_2 generated (reference)\n",
      "TFIDF wordcloud for ngram_range 2_2 generated (reference)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import file_manager\n",
    "\n",
    "file_manager.clear_path(\"./vectorizers/\")\n",
    "file_manager.clear_path(\"./img/\")\n",
    "# Define the ranges of n_grams that we want to use\n",
    "n_grams = [[(1, 1), (1, 2), (2, 2)],[]]\n",
    "for ngram_range in n_grams[0]:\n",
    "    formatted_range = str(ngram_range).strip(\"(\").strip(\")\").replace(\" \", \"\").replace(\",\", \"_\")\n",
    "    n_grams[1].append(formatted_range)\n",
    "\n",
    "for i, ngram_range in enumerate(n_grams[0]):\n",
    "   vectorize(ngram_range, n_grams[1][i], df_train, df_test)\n",
    "   if generate_wordcloud:\n",
    "       vectorize(ngram_range, n_grams[1][i], df_train_depression, None, \"depression\")\n",
    "       vectorize(ngram_range, n_grams[1][i], df_train_control, None, \"reference\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "np.save(\"./vectorizers/labels_train.npy\", df_train[\"depression_related\"])\n",
    "np.save(\"./vectorizers/labels_test.npy\", df_test[\"depression_related\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "if reset_variables:\n",
    "    %reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# %reset -f\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit0e230da09833446789e7a9e442994782"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}