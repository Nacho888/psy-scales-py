{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reset_variables = False\n",
    "generate_wordcloud = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training datasets...\n",
      "Loading testing datasets...\n",
      "Datasets succesfully loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Introduce the *paths* of the datasets to be preprocessed\n",
    "\n",
    "print(\"Loading training datasets...\")\n",
    "df_train_depression = pd.DataFrame(pd.read_json(\"../datasets/training/raw_dep_authors_training_cut.jsonl\", lines=True))\n",
    "df_train_control = pd.DataFrame(pd.read_json(\"../datasets/training/raw_ctrl_authors_training_cut.jsonl\", lines=True))\n",
    "\n",
    "print(\"Loading testing datasets...\")\n",
    "df_test_depression = pd.DataFrame(pd.read_json(\"../datasets/testing/raw_dep_authors_testing_cut.jsonl\", lines=True))\n",
    "df_test_control = pd.DataFrame(pd.read_json(\"../datasets/testing/raw_ctrl_authors_testing_cut.jsonl\", lines=True))\n",
    "\n",
    "print(\"Datasets succesfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Join dataframes\n",
    "df_train = df_train_depression.append(df_train_control, ignore_index=True)\n",
    "df_test = df_test_depression.append(df_test_control, ignore_index=True)\n",
    "\n",
    "if not generate_wordcloud:\n",
    "    # Remove old dataframes to preserve memory\n",
    "    del df_train_depression, df_train_control, df_test_depression, df_test_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Substitute reddit's keyword '[removed]'\n",
    "df_train[\"title\"] = np.where((df_train.title == \"[removed]\"),'', df_train.title)\n",
    "df_train[\"selftext\"] = np.where((df_train.selftext == \"[removed]\"),'', df_train.selftext)\n",
    "df_test[\"title\"] = np.where((df_test.title == \"[removed]\"),'', df_test.title)\n",
    "df_test[\"selftext\"] = np.where((df_test.selftext == \"[removed]\"),'', df_test.selftext)\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression[\"title\"] = np.where((df_train_depression.title == \"[removed]\"),'', df_train_depression.title)\n",
    "    df_train_depression[\"selftext\"] = np.where((df_train_depression.selftext == \"[removed]\"),'', df_train_depression.selftext)\n",
    "    df_train_control[\"title\"] = np.where((df_train_control.title == \"[removed]\"),'', df_train_control.title)\n",
    "    df_train_control[\"selftext\"] = np.where((df_train_control.selftext == \"[removed]\"),'', df_train_control.selftext)\n",
    "\n",
    "# Remove rows with no text in title neither selftext\n",
    "df_train = df_train[df_train[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "df_test = df_test[df_test[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression = df_train_depression[df_train_depression[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "    df_train_control = df_train_control[df_train_control[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "\n",
    "# Join the rest\n",
    "df_train[\"text\"] = df_train[\"title\"] + \" \"  + df_train[\"selftext\"]\n",
    "df_test[\"text\"] = df_test[\"title\"] + \" \" + df_test[\"selftext\"]\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression[\"text\"] = df_train_depression[\"title\"] + \" \"  + df_train_depression[\"selftext\"]\n",
    "    df_train_control[\"text\"] = df_train_control[\"title\"] + \" \" + df_train_control[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=13838.0, style=ProgressStyle(descripti…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87f21d1186364a55b3ae10ea3ce9c2f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=2968.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "514979fcb1b745769f761867b5f8652d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=7512.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68fadb59de19475c8c1a623a3282a6f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=6326.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ebf6c1869e84b919d64d7a8209bd3d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import cleantext\n",
    "import re\n",
    "import swifter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "\n",
    "try:\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords_set) + r')\\b\\s*')\n",
    "\n",
    "def pre_process(text: str):\n",
    "    \"\"\"\n",
    "    Given a string cleans it up and returns it once processed. Applies:\n",
    "    \n",
    "    1) Lowercase\n",
    "    2) Punctuation removal\n",
    "    3) Conversion of unicode symbols\n",
    "    4) Normalize whitespaces\n",
    "    5) Stopwords removal (using nltk's english list)\n",
    "    6) Stem words using a semi-aggressive stemmer (the classical Porter Stemmer)\n",
    "    \n",
    "    :param text: str - the string to be processed\n",
    "    :return: str - the processed string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lowercase, remove punctuation, convert unicode symbols and normalize whitespaces\n",
    "    processed = cleantext.clean(text, lower=True, fix_unicode=True, no_punct=True, no_urls=True)\n",
    "\n",
    "    # Remove stopwords\n",
    "    processed = pattern.sub('', processed)\n",
    "\n",
    "    # Stem words\n",
    "    try:\n",
    "        stemmed_words = [pst.stem(word) for word in word_tokenize(processed)]\n",
    "    except LookupError:\n",
    "        import nltk\n",
    "        nltk.download(\"punkt\")\n",
    "        stemmed_words = [pst.stem(word) for word in word_tokenize(processed)]\n",
    "\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].swifter.apply(lambda x: pre_process(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].swifter.apply(lambda x: pre_process(x))\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression[\"text\"] = df_train_depression[\"text\"].swifter.apply(lambda x: pre_process(x))\n",
    "    df_train_control[\"text\"] = df_train_control[\"text\"].swifter.apply(lambda x: pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset reduction to only text\n",
    "df_train = df_train[[\"text\", \"depression_related\"]]\n",
    "df_test = df_test[[\"text\", \"depression_related\"]]\n",
    "\n",
    "if generate_wordcloud:\n",
    "    df_train_depression = df_train_depression[[\"text\", \"depression_related\"]]\n",
    "    df_train_control = df_train_control[[\"text\", \"depression_related\"]]\n",
    "\n",
    "df_train.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                 path_or_buf=\"../datasets/training/proc_training.jsonl\")\n",
    "df_test.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                 path_or_buf=\"../datasets/testing/proc_testing.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def vectorize(ngram, step_name, train, test):\n",
    "    # Bag-of-words\n",
    "    begin_time = datetime.datetime.now()\n",
    "    cv = CountVectorizer(ngram_range=ngram, max_features=10000)\n",
    "    train_bow = cv.fit_transform(train.text.to_list())\n",
    "\n",
    "    if generate_wordcloud:\n",
    "        freqs = dict(zip(cv.get_feature_names(), train_bow.toarray().sum(axis=0)))\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(freqs)\n",
    "        wordcloud.to_file(\"./img/depression_bow_{}.png\".format(step_name))\n",
    "    else:\n",
    "        test_bow = cv.transform(test.text.tolist())\n",
    "        # Save both the train and test arrays generated and the vectorizer used\n",
    "        sparse.save_npz(\"./vectorizers/train_bow_{}.npz\".format(step_name), train_bow)\n",
    "        sparse.save_npz(\"./vectorizers/test_bow_{}.npz\".format(step_name), test_bow)\n",
    "        joblib.dump(cv, \"./vectorizers/vect_bow_{}\".format(step_name))\n",
    "\n",
    "    print(\"Shape of BOW vector: {}\".format(train_bow.shape))\n",
    "    print(\"\\tTotal elapsed time (BOW): {} for ngram_range {}\".format(datetime.datetime.now() - begin_time, ngram_range))\n",
    "\n",
    "    # Term frequency - inverse term frequency\n",
    "    begin_time = datetime.datetime.now()\n",
    "    tf_trans = TfidfTransformer()\n",
    "    train_tfidf = tf_trans.fit_transform(train_bow)\n",
    "\n",
    "    if generate_wordcloud:\n",
    "        freqs = dict(zip(cv.get_feature_names(), train_tfidf.toarray().sum(axis=0)))\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(freqs)\n",
    "        wordcloud.to_file(\"./img/reference_tfidf_{}.png\".format(step_name))\n",
    "    else:\n",
    "        test_tfidf = tf_trans.transform(test_bow)\n",
    "        # Save both the train and test arrays generated and the vectorizer used\n",
    "        sparse.save_npz(\"./vectorizers/train_tfidf_{}.npz\".format(step_name), train_tfidf)\n",
    "        sparse.save_npz(\"./vectorizers/test_tfidf_{}.npz\".format(step_name), test_tfidf)\n",
    "        joblib.dump(tf_trans, \"./vectorizers/vect_tfidf_{}\".format(step_name))\n",
    "\n",
    "    print(\"Shape of TFIDF vector: {}\".format(train_tfidf.shape))\n",
    "    print(\"\\tTotal elapsed time (TFIDF): {} for ngram_range {}\\n\".format(datetime.datetime.now() - begin_time, ngram_range))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BOW vector: (13838, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:01.232580 for ngram_range (1, 1)\n",
      "Shape of TFIDF vector: (13838, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.600098 for ngram_range (1, 1)\n",
      "\n",
      "Shape of BOW vector: (7512, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:00.771939 for ngram_range (1, 1)\n",
      "Shape of TFIDF vector: (7512, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.419431 for ngram_range (1, 1)\n",
      "\n",
      "Shape of BOW vector: (13838, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:03.369066 for ngram_range (1, 2)\n",
      "Shape of TFIDF vector: (13838, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.636576 for ngram_range (1, 2)\n",
      "\n",
      "Shape of BOW vector: (7512, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:01.929534 for ngram_range (1, 2)\n",
      "Shape of TFIDF vector: (7512, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.430164 for ngram_range (1, 2)\n",
      "\n",
      "Shape of BOW vector: (13838, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:02.963905 for ngram_range (2, 2)\n",
      "Shape of TFIDF vector: (13838, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.615559 for ngram_range (2, 2)\n",
      "\n",
      "Shape of BOW vector: (7512, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:01.768608 for ngram_range (2, 2)\n",
      "Shape of TFIDF vector: (7512, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.470427 for ngram_range (2, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import file_manager\n",
    "\n",
    "file_manager.clear_path(\"./vectorizers/\")\n",
    "file_manager.clear_path(\"./img/\")\n",
    "# Define the ranges of n_grams that we want to use\n",
    "n_grams = [[(1, 1), (1, 2), (2, 2)],[]]\n",
    "for ngram_range in n_grams[0]:\n",
    "    formatted_range = str(ngram_range).strip(\"(\").strip(\")\").replace(\" \", \"\").replace(\",\", \"_\")\n",
    "    n_grams[1].append(formatted_range)\n",
    "\n",
    "for i, ngram_range in enumerate(n_grams[0]):\n",
    "   vectorize(ngram_range, n_grams[1][i], df_train, df_test)\n",
    "   if generate_wordcloud:\n",
    "       vectorize(ngram_range, n_grams[1][i], df_train_depression, df_train_depression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./vectorizers/labels_train\", df_train[\"depression_related\"])\n",
    "np.save(\"./vectorizers/labels_test\", df_test[\"depression_related\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if reset_variables:\n",
    "    %reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit0e230da09833446789e7a9e442994782"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}