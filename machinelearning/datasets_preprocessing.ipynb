{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training datasets...\n",
      "Loading testing datasets...\n",
      "Datasets succesfully loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Introduce the *paths* of the datasets to be preprocessed\n",
    "\n",
    "print(\"Loading training datasets...\")\n",
    "df_train_depression = pd.DataFrame(pd.read_json(\"../datasets/training/raw_dep_authors_training_cut.jsonl\", lines=True))\n",
    "df_train_control = pd.DataFrame(pd.read_json(\"../datasets/training/raw_ctrl_authors_training_cut.jsonl\", lines=True))\n",
    "\n",
    "print(\"Loading testing datasets...\")\n",
    "df_test_depression = pd.DataFrame(pd.read_json(\"../datasets/testing/raw_dep_authors_testing_cut.jsonl\", lines=True))\n",
    "df_test_control = pd.DataFrame(pd.read_json(\"../datasets/testing/raw_ctrl_authors_testing_cut.jsonl\", lines=True))\n",
    "\n",
    "print(\"Datasets succesfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Join dataframes\n",
    "df_train = df_train_depression.append(df_train_control, ignore_index=True)\n",
    "df_test = df_test_depression.append(df_test_control, ignore_index=True)\n",
    "\n",
    "# Remove old dataframes to preserve memory\n",
    "del df_train_depression, df_train_control, df_test_depression, df_test_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Substitute reddit's keyword '[removed]'\n",
    "df_train[\"title\"] = np.where((df_train.title == \"[removed]\"),'', df_train.title)\n",
    "df_train[\"selftext\"] = np.where((df_train.selftext == \"[removed]\"),'', df_train.selftext)\n",
    "df_test[\"title\"] = np.where((df_test.title == \"[removed]\"),'', df_test.title)\n",
    "df_test[\"selftext\"] = np.where((df_test.selftext == \"[removed]\"),'', df_test.selftext)\n",
    "\n",
    "# Remove rows with no text in title neither selftext\n",
    "df_train = df_train[df_train[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "df_test = df_test[df_test[[\"title\", \"selftext\"]].ne('').all(axis=1)]\n",
    "\n",
    "# Join the rest\n",
    "df_train[\"text\"] = df_train[\"title\"] + \" \"  + df_train[\"selftext\"]\n",
    "df_test[\"text\"] = df_test[\"title\"] + \" \" + df_test[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=7145.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a94701c0d67a4d6790e4abfd7392e400"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=2846.0, style=ProgressStyle(descriptio…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56016bf02f9841b2a10c0bb8f719a31f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import cleantext\n",
    "import re\n",
    "import swifter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords_set) + r')\\b\\s*')\n",
    "\n",
    "def pre_process(text: str):\n",
    "    \"\"\"\n",
    "    Given a string cleans it up and returns it once processed. Applies:\n",
    "    \n",
    "    1) Lowercase\n",
    "    2) Punctuation removal\n",
    "    3) Conversion of unicode symbols\n",
    "    4) Normalize whitespaces\n",
    "    5) Stopwords removal (using nltk's english list)\n",
    "    6) Stem words using a semi-aggressive stemmer (the classical Porter Stemmer)\n",
    "    \n",
    "    :param text: str - the string to be processed\n",
    "    :return: str - the processed string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lowercase, remove punctuation, convert unicode symbols and normalize whitespaces\n",
    "    processed = cleantext.clean(text, lower=True, fix_unicode=True, no_punct=True, no_urls=True)\n",
    "    # Remove stopwords\n",
    "    processed = pattern.sub('', processed)\n",
    "    # Stem words\n",
    "    stemmed_words = [pst.stem(word) for word in word_tokenize(processed)]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].swifter.apply(lambda x: pre_process(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].swifter.apply(lambda x: pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Dataset reduction to only text\n",
    "df_train = df_train[[\"text\", \"depression_related\"]]\n",
    "df_test = df_test[[\"text\", \"depression_related\"]]\n",
    "\n",
    "df_train.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                 path_or_buf=\"../datasets/training/proc_training.jsonl\")\n",
    "df_test.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                 path_or_buf=\"../datasets/testing/proc_testing.jsonl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BOW vector: (7145, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:00.959167 for ngram_range (1, 1)\n",
      "Shape of TFIDF vector: (7145, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.297101 for ngram_range (1, 1)\n",
      "\n",
      "Shape of BOW vector: (7145, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:03.039072 for ngram_range (1, 2)\n",
      "Shape of TFIDF vector: (7145, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.322139 for ngram_range (1, 2)\n",
      "\n",
      "Shape of BOW vector: (7145, 10000)\n",
      "\tTotal elapsed time (BOW): 0:00:02.402268 for ngram_range (2, 2)\n",
      "Shape of TFIDF vector: (7145, 10000)\n",
      "\tTotal elapsed time (TFIDF): 0:00:00.080156 for ngram_range (2, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import file_manager\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "file_manager.clear_path(\"./vectorizers\")\n",
    "# Define the ranges of n_grams that we want to use\n",
    "n_grams = [[(1, 1), (1, 2), (2, 2)],[]]\n",
    "for ngram_range in n_grams[0]:\n",
    "    formatted_range = str(ngram_range).strip(\"(\").strip(\")\").replace(\" \", \"\").replace(\",\", \"_\")\n",
    "    n_grams[1].append(formatted_range)\n",
    "\n",
    "for i, ngram_range in enumerate(n_grams[0]):\n",
    "    # Bag-of-words\n",
    "    begin_time = datetime.datetime.now()\n",
    "    cv = CountVectorizer(ngram_range=ngram_range, max_features=10000)\n",
    "    train_bow = cv.fit_transform(df_train.text.to_list())\n",
    "    test_bow = cv.transform(df_test.text.tolist())\n",
    "\n",
    "    # Save both the train and test arrays generated and the vectorizer used\n",
    "    sparse.save_npz(\"./vectorizers/train_bow_{}.npz\".format(n_grams[1][i]), train_bow)\n",
    "    sparse.save_npz(\"./vectorizers/test_bow_{}.npz\".format(n_grams[1][i]), test_bow)\n",
    "    joblib.dump(cv, \"./vectorizers/vect_bow_{}\".format(n_grams[1][i]))\n",
    "\n",
    "    print(\"Shape of BOW vector: {}\".format(train_bow.shape))\n",
    "    print(\"\\tTotal elapsed time (BOW): {} for ngram_range {}\".format(datetime.datetime.now() - begin_time, ngram_range))\n",
    "\n",
    "    # Term frequency - inverse term frequency\n",
    "    begin_time = datetime.datetime.now()\n",
    "    tf_trans = TfidfTransformer()\n",
    "    train_tfidf = tf_trans.fit_transform(train_bow)\n",
    "    test_tfidf = tf_trans.transform(test_bow)\n",
    "\n",
    "    # Save both the train and test arrays generated and the vectorizer used\n",
    "    sparse.save_npz(\"./vectorizers/train_tfidf_{}.npz\".format(n_grams[1][i]), train_tfidf)\n",
    "    sparse.save_npz(\"./vectorizers/test_tfidf_{}.npz\".format(n_grams[1][i]), test_tfidf)\n",
    "    joblib.dump(tf_trans, \"./vectorizers/vect_tfidf_{}\".format(n_grams[1][i]))\n",
    "\n",
    "    print(\"Shape of TFIDF vector: {}\".format(train_tfidf.shape))\n",
    "    print(\"\\tTotal elapsed time (TFIDF): {} for ngram_range {}\\n\".format(datetime.datetime.now() - begin_time, ngram_range))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./vectorizers/labels_train\", df_train[\"depression_related\"])\n",
    "np.save(\"./vectorizers/labels_test\", df_test[\"depression_related\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38164bit0e230da09833446789e7a9e442994782",
   "language": "python",
   "display_name": "Python 3.8.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}