{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "reset_variables = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check ML folder structure\n",
    "base = \"datasets\"\n",
    "subdirs = [\"training\", \"testing\"]\n",
    "\n",
    "if not os.path.isdir(\"../datasets/\"):\n",
    "    os.mkdir(\"../datasets/\")\n",
    "for subdir in subdirs:\n",
    "    if not os.path.isdir(\"../datasets/{}/\".format(subdir)):\n",
    "        os.mkdir(\"../datasets/{}/\".format(subdir))\n",
    "\n",
    "if not os.path.isdir(\"./vectorizers/\"):\n",
    "    os.mkdir(\"./vectorizers/\")\n",
    "if not os.path.isdir(\"./img/\"):\n",
    "    os.mkdir(\"./img/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw datasets...\n",
      "Raw datasets succesfully loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset files\n",
    "print(\"Loading raw datasets...\")\n",
    "\n",
    "# Introduce the *paths* of the raw datasets\n",
    "\n",
    "# Depression\n",
    "data_depression = pd.DataFrame(pd.read_json(\"../backups/r_depression_base.jsonl\", lines=True))\n",
    "data_depression[\"depression_related\"] = [1] * len(data_depression.index)  # Dep. identifier: true\n",
    "dep_size = len(data_depression.index)\n",
    "\n",
    "# Non-depression\n",
    "data_control = pd.DataFrame(pd.read_json(\"../backups/reference_collection.jsonl\", lines=True))\n",
    "data_control[\"depression_related\"] = [0] * len(data_control.index)  # Dep. identifier: false\n",
    "non_dep_size = len(data_control.index)\n",
    "\n",
    "print(\"Raw datasets succesfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Only to execute if the authors have been further preprocessed using the notebooks 'authors_preprocessing'\n",
    "\n",
    "import tools\n",
    "\n",
    "clean_authors, clean_subreddits = False, False\n",
    "cleaned = False\n",
    "\n",
    "authors_dep = pd.read_excel(\"../data/cleaned_authors_180.xlsx\")\n",
    "remove_subreddits = tools.list_excluded_subreddits(\"../data/dep_subreddits.txt\", [\"depression\"])\n",
    "\n",
    "if clean_authors:\n",
    "    data_depression = data_depression[data_depression[\"author\"].isin(authors_dep[\"username\"].tolist()[0::2])]\n",
    "    data_control = data_control[data_control[\"author\"].isin(authors_dep[\"username\"].tolist()[1::2])]\n",
    "    cleaned = True\n",
    "if clean_subreddits:\n",
    "    data_depression = data_depression[~data_depression[\"subreddit\"].isin(remove_subreddits)]\n",
    "    data_control = data_control[~data_control[\"subreddit\"].isin(remove_subreddits)]\n",
    "    cleaned = True\n",
    "\n",
    "if cleaned:\n",
    "    data_depression.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                                         path_or_buf=\"../datasets/raw_dep_cleaned.jsonl\")\n",
    "    data_control.to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                                         path_or_buf=\"../datasets/raw_ctrl_cleaned.jsonl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut off date: 2019-04-16 18:57:09\n",
      "Cut off ID for control: bdxkhq\n",
      "Cut off ID for depression: bdxph5\n"
     ]
    }
   ],
   "source": [
    "# Percentage of posts that we want to be in the test set\n",
    "# Datasets should be ordered in descending order of date (created_utc)\n",
    "percentage = 20\n",
    "cut_off_row = data_depression.head(int(len(data_depression) * (percentage / 100))).tail(1)\n",
    "cut_off_date, cut_off_id = cut_off_row[\"created_utc\"].iloc[0], cut_off_row[\"id\"].iloc[0]\n",
    "control_mask, depression_mask = data_control[\"created_utc\"] <= cut_off_date, \\\n",
    "                                data_depression[\"created_utc\"] <= cut_off_date\n",
    "\n",
    "print(\"Cut off date: {}\".format(pd.to_datetime(cut_off_date, unit=\"s\")))\n",
    "print(\"Cut off ID for control: {}\".format(data_control[control_mask].head(1)[\"id\"].iloc[0]))\n",
    "print(\"Cut off ID for depression: {}\".format(cut_off_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training/testing datasets generated\n"
     ]
    }
   ],
   "source": [
    "# Introduce the *paths* to save the cut datasets\n",
    "\n",
    "# Save the datasets with all the features (all columns)\n",
    "data_depression[depression_mask].to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                                         path_or_buf=\"../datasets/training/dep_training_full.jsonl\")\n",
    "data_depression[~depression_mask].to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                                          path_or_buf=\"../datasets/testing/dep_testing_full.jsonl\")\n",
    "\n",
    "data_control[control_mask].to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                                   path_or_buf=\"../datasets/training/ctrl_training_full.jsonl\")\n",
    "data_control[~control_mask].to_json(orient=\"records\", lines=True, force_ascii=True,\n",
    "                                    path_or_buf=\"../datasets/testing/ctrl_testing_full.jsonl\")\n",
    "\n",
    "print(\"Raw training/testing datasets generated\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed training/testing datasets generated\n"
     ]
    }
   ],
   "source": [
    "# Introduce the *paths* to save the cut datasets\n",
    "\n",
    "# Save the datasets with only the data required for our task of text classification\n",
    "data_depression[[\"title\", \"selftext\", \"depression_related\"]][depression_mask].to_json(orient=\"records\", lines=True,\n",
    "                                                                                      force_ascii=True,\n",
    "                                                                                      path_or_buf=\"../datasets/training/dep_training_cut.jsonl\")\n",
    "data_depression[[\"title\", \"selftext\", \"depression_related\"]][~depression_mask].to_json(orient=\"records\", lines=True,\n",
    "                                                                                       force_ascii=True,\n",
    "                                                                                       path_or_buf=\"../datasets/testing/dep_testing_cut.jsonl\")\n",
    "\n",
    "data_control[[\"title\", \"selftext\", \"depression_related\"]][control_mask].to_json(orient=\"records\", lines=True,\n",
    "                                                                                force_ascii=True,\n",
    "                                                                                path_or_buf=\"../datasets/training/ctrl_training_cut.jsonl\")\n",
    "data_control[[\"title\", \"selftext\", \"depression_related\"]][~control_mask].to_json(orient=\"records\", lines=True,\n",
    "                                                                                 force_ascii=True,\n",
    "                                                                                 path_or_buf=\"../datasets/testing/ctrl_testing_cut.jsonl\")\n",
    "\n",
    "print(\"Processed training/testing datasets generated\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if reset_variables:\n",
    "    %reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit0e230da09833446789e7a9e442994782"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}