{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3The first thing to be done is to gather the data from out datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training datasets...\n",
      "Datasets succesfully loaded and merged\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset files\n",
    "print(\"Loading training datasets...\")\n",
    "\n",
    "# Depression (train)\n",
    "train_data_depression = pd.DataFrame(pd.read_json(\"../datasets/training/training_depression_40000.jsonl\", lines=True))\n",
    "train_data_depression[\"depression_related\"] = [1] * len(train_data_depression.index)  # Dep. identifier: true\n",
    "dep_size = len(train_data_depression.index)\n",
    "\n",
    "# Non-depression (train)\n",
    "train_data_control = pd.DataFrame(pd.read_json(\"../datasets/training/training_control_40000.jsonl\", lines=True))\n",
    "train_data_control[\"depression_related\"] = [0] * len(train_data_control.index)  # Dep. identifier: false\n",
    "non_dep_size = len(train_data_control.index)\n",
    "\n",
    "# Join both data-frames\n",
    "train = train_data_depression.append(train_data_control, ignore_index=True)\n",
    "print(\"Datasets succesfully loaded and merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After that we have to work with our dataframe to have the correct data to work with. We will need:\n",
    "* The text in the title and in the selftext of each post\n",
    "* The proportion of pronouns in each post\n",
    "* The hour of posting\n",
    "* The month of posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the texts\n",
    "try:\n",
    "    train[\"selftext\"]\n",
    "except KeyError:\n",
    "    train[\"text\"] = train[\"title\"]\n",
    "else:\n",
    "    train[\"text\"] = train[\"title\"] + train[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:382: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-a2debded2059>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature_extraction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mCountVectorizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mcv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCountVectorizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mngram_range\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_words\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"english\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlowercase\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpreprocessor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmlu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpre_process\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mcv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36mfit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1217\u001B[0m         \u001B[0mmax_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1218\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1219\u001B[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001B[0m\u001B[0;32m   1220\u001B[0m                                           self.fixed_vocabulary_)\n\u001B[0;32m   1221\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1129\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mraw_documents\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1130\u001B[0m             \u001B[0mfeature_counter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1131\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[1;32min\u001B[0m \u001B[0manalyze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1132\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1133\u001B[0m                     \u001B[0mfeature_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfeature\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mdecoder\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 98\u001B[1;33m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdecoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     99\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0manalyzer\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    100\u001B[0m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0manalyzer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, doc)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 218\u001B[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001B[0m\u001B[0;32m    219\u001B[0m                              \"unicode string.\")\n\u001B[0;32m    220\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "import mlearning_utils as mlu\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2), stop_words=\"english\", lowercase=True, preprocessor=mlu.pre_process)\n",
    "cv.fit_transform(train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-be03b628d8ae>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m# Pronouns proportion\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"pp1\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmlu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_pronoun_proportion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"pp1\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"pp2\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmlu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_pronoun_proportion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"pp2\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"pp3\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmlu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_pronoun_proportion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"pp3\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self, func, convert_dtype, args, **kwds)\u001B[0m\n\u001B[0;32m   3846\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3847\u001B[0m                 \u001B[0mvalues\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobject\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3848\u001B[1;33m                 \u001B[0mmapped\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_infer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconvert\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconvert_dtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3850\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmapped\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmapped\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSeries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m<ipython-input-9-be03b628d8ae>\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;31m# Pronouns proportion\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"pp1\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmlu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_pronoun_proportion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"pp1\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"pp2\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmlu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_pronoun_proportion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"pp2\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"pp3\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmlu\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_pronoun_proportion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"pp3\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001B[0m in \u001B[0;36mword_tokenize\u001B[1;34m(text, language, preserve_line)\u001B[0m\n\u001B[0;32m    142\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[0mtype\u001B[0m \u001B[0mpreserve_line\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m     \"\"\"\n\u001B[1;32m--> 144\u001B[1;33m     \u001B[0msentences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mpreserve_line\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0msent_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlanguage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m     return [\n\u001B[0;32m    146\u001B[0m         \u001B[0mtoken\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msentences\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_treebank_word_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msent\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001B[0m in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m    104\u001B[0m     \"\"\"\n\u001B[0;32m    105\u001B[0m     \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'tokenizers/punkt/{0}.pickle'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlanguage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 106\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1275\u001B[0m         \u001B[0mGiven\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturns\u001B[0m \u001B[0ma\u001B[0m \u001B[0mlist\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0msentences\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mthat\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1276\u001B[0m         \"\"\"\n\u001B[1;32m-> 1277\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msentences_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1278\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1279\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdebug_decisions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36msentences_from_text\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1329\u001B[0m         \u001B[0mfollows\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mperiod\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1330\u001B[0m         \"\"\"\n\u001B[1;32m-> 1331\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1332\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1333\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1329\u001B[0m         \u001B[0mfollows\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mperiod\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1330\u001B[0m         \"\"\"\n\u001B[1;32m-> 1331\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mspan_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1332\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1333\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36mspan_tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1319\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mrealign_boundaries\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1320\u001B[0m             \u001B[0mslices\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_realign_boundaries\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mslices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1321\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0msl\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mslices\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1322\u001B[0m             \u001B[1;32myield\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1323\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_realign_boundaries\u001B[1;34m(self, text, slices)\u001B[0m\n\u001B[0;32m   1360\u001B[0m         \"\"\"\n\u001B[0;32m   1361\u001B[0m         \u001B[0mrealign\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1362\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0msl1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl2\u001B[0m \u001B[1;32min\u001B[0m \u001B[0m_pair_iter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mslices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1363\u001B[0m             \u001B[0msl1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mslice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msl1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mrealign\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msl1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1364\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0msl2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_pair_iter\u001B[1;34m(it)\u001B[0m\n\u001B[0;32m    316\u001B[0m     \u001B[0mit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    317\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 318\u001B[1;33m         \u001B[0mprev\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    319\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    320\u001B[0m         \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001B[0m in \u001B[0;36m_slices_from_text\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m   1333\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_slices_from_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1334\u001B[0m         \u001B[0mlast_break\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1335\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mmatch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lang_vars\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mperiod_context_re\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinditer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1336\u001B[0m             \u001B[0mcontext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mmatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'after_tok'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1337\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext_contains_sentbreak\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcontext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Pronouns proportion\n",
    "train[\"pp1\"] = train[\"text\"].apply(lambda x: mlu.get_pronoun_proportion(word_tokenize(x), \"pp1\"))\n",
    "train[\"pp2\"] = train[\"text\"].apply(lambda x: mlu.get_pronoun_proportion(word_tokenize(x), \"pp2\"))\n",
    "train[\"pp3\"] = train[\"text\"].apply(lambda x: mlu.get_pronoun_proportion(word_tokenize(x), \"pp3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import date_utils\n",
    "# Hour and month\n",
    "train[\"hour\"] = train[\"created_utc\"].apply(lambda x: date_utils.extract_field_from_timestamp(x, \"hour\"))\n",
    "train[\"month\"] = train[\"created_utc\"].apply(lambda x: date_utils.extract_field_from_timestamp(x, \"month\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit0e230da09833446789e7a9e442994782"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}